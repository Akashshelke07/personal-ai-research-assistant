

# Personal AI Research Assistant (Local)

A **100% local, privacy-focused AI research assistant** that can answer questions about your own documents using a **Retrieval-Augmented Generation (RAG)** pipeline.

All processing â€” including the database, vector search, and AI inference â€” happens **entirely on your machine**. No internet calls, no API keys, no data leaks.

---

## âœ¨ Features

* ğŸ“š **Document Ingestion** â€” Supports **PDF (.pdf)**, **Markdown (.md)**, and **Text (.txt)** files.
* ğŸ§  **Local Question Answering** â€” Powered by a local LLM (no API needed).
* ğŸŒ **Simple Web Interface** â€” Easy-to-use browser UI.
* ğŸ—£ **Text-to-Speech** â€” Convert answers to speech with one click.
* ğŸ”’ **Completely Private** â€” Nothing leaves your machine.
* âš™ **Extensible** â€” Modular, service-based architecture for easy feature additions.

---

## ğŸ›  Tech Stack

**Backend:** Python, FastAPI, Uvicorn
**AI / ML:** LangChain, GPT4All (GGUF models, e.g., Mistral), Sentence Transformers (Hugging Face), PyTorch
**Vector Database:** ChromaDB
**Text-to-Speech:** pyttsx3
**Frontend:** HTML, CSS, JavaScript

---

## ğŸ“‚ Project Structure

```
personal-ai-research-assistant/
â”œâ”€â”€ data/                  # Your .pdf, .md, and .txt files
â”œâ”€â”€ models/                # Local LLM model file
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ingestion/         # Document loading and database creation
â”‚   â”œâ”€â”€ langchain_api/     # FastAPI backend
â”‚   â”œâ”€â”€ tts/               # Text-to-speech logic
â”‚   â””â”€â”€ web/               # Frontend (HTML/CSS/JS)
â”œâ”€â”€ .env                   # Configuration file (create this)
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ run_all.py             # Main startup script
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/YourUsername/personal-ai-research-assistant.git
cd personal-ai-research-assistant
```

### 2ï¸âƒ£ Set Up a Virtual Environment

```bash
python -m venv .venv
# Activate (Windows Git Bash)
source .venv/Scripts/activate
# Activate (Linux/Mac)
source .venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download the AI Model

Download **mistral-7b-openorca.Q4\_0.gguf**:
ğŸ”— [Download Link](https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf)

Place it in the `models/` folder.

### 5ï¸âƒ£ Configure Environment Variables

Create a `.env` file in the project root:

```env
# --- ChromaDB Settings ---
CHROMA_DIR=./chroma_db
CHROMA_COLLECTION_NAME=personal_ai_research_assistant

# --- Embedding Model ---
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# --- Local LLM Settings ---
LLM_MODEL_PATH=./models/mistral-7b-openorca.Q4_0.gguf
```

### 6ï¸âƒ£ Ingest Your Documents

Place `.pdf`, `.txt`, or `.md` files into `data/`. Then run:

```bash
python services/ingestion/ingest.py
```

---

## â–¶ Running the Application

```bash
python run_all.py
```

This starts Uvicorn at **[http://0.0.0.0:8000](http://0.0.0.0:8000)**.

---

## ğŸ’¡ Usage

### **Web Interface** (Recommended)

1. Open `services/web/index.html` in your browser.
2. Type a question and click **Ask**.
3. Click **Speak Answer** to hear it aloud.

*(Tip: In VS Code, right-click `index.html` â†’ **Open with Live Server** for instant reload.)*

### **API**

Example using `curl`:

```bash
curl -X POST "http://127.0.0.1:8000/ask" \
-H "Content-Type: application/json" \
-d '{"query": "What is the main topic of the sample research?"}'
```

Do you want me to also **add screenshots and example outputs** so this README looks more appealing on GitHub?
